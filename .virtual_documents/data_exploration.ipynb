





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from tqdm import tqdm

from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
from transformers import pipeline
from scipy.special import softmax


import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

#Setting pandas display options
pd.set_option("max_colwidth", None)


from sentence_transformers import SentenceTransformer, util


#Src folder path
src_folder = 'src/data/'


df_plot_summaries = pd.read_csv(src_folder + 'plot_summaries.txt', sep='\t', header=None,  names=['id', 'summary'])
df_plot_summaries.sample(2)





def clean_plot(txt):

    #Remove URLs
    txt = re.sub(r"http\S+|www\.\S+", '', txt)

    #Remove HTML tags
    txt = re.sub(r'<.*?>', '', txt)

    #Remove {{annotations}}
    txt = re.sub(r'\{\{.*?\}\}', '', txt)

    #Remove the ([[ annotation that is never closed
    txt = re.sub(r'\(\[\[', '', txt)

    #Remove the synopsis from context
    txt = re.sub(r'Synopsis from', '', txt)

    #Remove <ref...}} tags
    txt = re.sub(r'<ref[^}]*}}', '', txt)

    return txt


df_test_clean = df_plot_summaries.copy()
df_test_clean["summary"] = df_plot_summaries['summary'].apply(clean_plot)
df_test_clean.sample(2)








df_length_summary = df_test_clean['summary'].copy(deep=True)
df_length_summary = df_length_summary.apply(len) 



df_length_summary.hist(bins=50, edgecolor='black')
plt.yscale('log')
plt.xlabel('Amount of word in the summary')
plt.ylabel('Amount of summary')
plt.title('Distribution of the length of the summary')


_, axes = plt.subplots(1, 2, figsize=(12, 6))
axes = axes.ravel()

df_length_summary_below_5000 = df_length_summary[df_length_summary <= 5000]
df_length_summary_below_5000.hist(bins=40, edgecolor='black', ax=axes[0])
axes[0].set_yscale('log')
axes[0].set_xlabel('Amount of word in the summary')
axes[0].set_ylabel('Amount of summary')
axes[0].set_title('Distribution of the summary that have length <= 5000')

thresh = 200
df_length_summary_below_1000 = df_length_summary[df_length_summary <= 200]
df_length_summary_below_1000.hist(bins=40, edgecolor='black', ax=axes[1])
axes[1].set_yscale('log')
axes[1].set_xlabel('Amount of word in the summary')
axes[1].set_ylabel('Amount of summary')
axes[1].set_title(f'Distribution of the summary that have length <= {thresh}')








#First possibility: sentence by sentence sentiment classification

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)



dict_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}
for t in df_test_clean['summary'].sample(1):
    for sentence in nltk.sent_tokenize(t):
        print(sentence)
        t_encoded = tokenizer(sentence, return_tensors='pt')
        t_output = model(**t_encoded)
        t_scores = softmax(t_output.logits.detach().numpy(), axis=1)
        t_predicted = np.argmax(t_scores)
        print(dict_labels[t_predicted], t_scores[0][t_predicted])
    


# Second possibility: sentence to sentence emotion classification

#the emotions are anger, fear, joy, love, sadness, surprise and neutral

classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)

for t in df_test_clean['summary'].sample(1):
    for sentence in nltk.sent_tokenize(t):
        print(sentence)
        out = classifier(sentence)[0]
        best_emotion_dict = max(out, key=lambda x: x['score'])
        best_label = best_emotion_dict['label']
        best_score = best_emotion_dict['score']
        print(best_label, best_score)    


#keep small summary to test the segmentation
df_test_clean_with_length = df_test_clean.copy(deep=True)
df_test_clean_with_length['length'] = df_test_clean_with_length['summary'].apply(len) 
df_test_clean_with_length = df_test_clean_with_length[df_test_clean_with_length['length'] < 150]
df_test_clean_with_length.head(5)


model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

df_summary = df_test_clean_with_length['summary']
# Exemple de résumé (ce texte serait votre résumé complet de film)
summary = """
The story begins with the protagonist arriving in a new town. He feels out of place at first, but soon makes friends. 
As the days pass, he gets entangled in local conflicts and starts to develop a romantic interest. 
This leads to a series of challenges that test his resolve. 
In the final showdown, he faces his ultimate challenge and emerges victorious, having found a new sense of belonging.
"""

# 1. Diviser le résumé en phrases
sentences = summary.split(". ")
sentences = [s.strip() for s in sentences if s]  # Supprimer les espaces et les phrases vides

# 2. Obtenir les embeddings de chaque phrase
embeddings = model.encode(sentences)

# 3. Calculer les similarités cosinus entre phrases successives
similarities = [util.pytorch_cos_sim(embeddings[i], embeddings[i+1]).item() for i in range(len(embeddings)-1)]

# 4. Identifier les points de transition basés sur un seuil de similarité
threshold = 0.5  # Par exemple, une similarité inférieure à 0.5 indique un changement de thème
segments = []
current_segment = [sentences[0]]

for i, similarity in enumerate(similarities):
    if similarity < threshold:  # Si la similarité est faible, cela peut indiquer une transition
        segments.append(" ".join(current_segment))  # Ajouter le segment actuel à la liste
        current_segment = []  # Réinitialiser pour un nouveau segment
    current_segment.append(sentences[i + 1])

segments.append(" ".join(current_segment))  # Ajouter le dernier segment

# 5. Afficher les segments
for idx, segment in enumerate(segments):
    print(f"Segment {idx+1}:\n{segment}\n")


# Second possibility: sentence to sentence emotion classification

#the emotions are anger, fear, joy, love, sadness, surprise and neutral

classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)

for t in df_test_clean['summary'].sample(1):
    for sentence in nltk.sent_tokenize(t):
        print(sentence)
        out = classifier(sentence)[0]
        best_emotion_dict = max(out, key=lambda x: x['score'])
        best_label = best_emotion_dict['label']
        best_score = best_emotion_dict['score']
        print(best_label, best_score)    


#try to segment the text in a more clever way I guess?
from sentence_transformers import SentenceTransformer, util
nltk.download('punkt')  # Télécharge le tokenizer nécessaire pour les phrases

# Initialisation du modèle de classification d'émotions
classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)

# Initialisation du modèle de similarité de phrases
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Exemple de résumé (ou utilisez df_test_clean_with_length['summary'] pour appliquer à tout le dataset)
summary = """
The story begins with the protagonist arriving in a new town. He feels out of place at first, but soon makes friends. 
As the days pass, he gets entangled in local conflicts and starts to develop a romantic interest. 
This leads to a series of challenges that test his resolve. 
In the final showdown, he faces his ultimate challenge and emerges victorious, having found a new sense of belonging.
"""
#summary = df_summary_box_office.iloc[3]['summary']

# 1. Diviser le résumé en phrases
sentences = nltk.sent_tokenize(summary)  # Utiliser nltk pour la tokenisation des phrases
sentences = [s.strip() for s in sentences if s]  # Supprimer les espaces et les phrases vides

# 2. Obtenir les embeddings de chaque phrase
embeddings = model.encode(sentences)

# 3. Calculer les similarités cosinus entre phrases successives
similarities = [util.pytorch_cos_sim(embeddings[i], embeddings[i+1]).item() for i in range(len(embeddings)-1)]

# 4. Identifier les points de transition basés sur un seuil de similarité
threshold = 0.5  # Par exemple, une similarité inférieure à 0.5 indique un changement de thème
segments = []
current_segment = [sentences[0]]

for i, similarity in enumerate(similarities):
    if similarity < threshold:  # Si la similarité est faible, cela peut indiquer une transition
        segments.append(" ".join(current_segment))  # Ajouter le segment actuel à la liste
        current_segment = []  # Réinitialiser pour un nouveau segment
    current_segment.append(sentences[i + 1])

segments.append(" ".join(current_segment))  # Ajouter le dernier segment

# 5. Analyser les émotions de chaque segment et afficher les résultats
best_emotions = []
emotionsS = []
for idx, segment in enumerate(segments):
    print(f"Segment {idx+1}:\n{segment}\n")
    
    # Classifier les émotions pour chaque segment
    emotions = classifier(segment)[0]
    emotionsS.append(emotions)
    best_emotion = max(emotions, key=lambda x: x['score'])  # Sélectionner l'émotion avec le score le plus élevé
    best_emotions.append(best_emotion)
    # Afficher l'émotion dominante pour le segment
    print(f"Emotion dominante pour le Segment {idx+1}: {best_emotion['label']} : {best_emotion['score']:.2f}\n")



emotions = np.array([best_em['label'] for best_em in best_emotions])
scores = np.array([best_em['score'] for best_em in best_emotions])




# Créer une liste de numéros de segment pour l'axe x
x = list(range(1, len(segments) + 1))

# Tracer les scores pour chaque segment
plt.figure(figsize=(8, 4))
plt.plot(x, scores, marker='o', linestyle='-', color='b', label='Score')

# Ajouter des annotations pour chaque point (émotion dominante)
for i, (emotion, score) in enumerate(zip(emotions, scores), start=1):
    plt.text(i, score + 0.02, f'{emotion}: {score:.2f}', ha='center', va='bottom')
# Configurer le graphique
plt.ylim(0, 1)
plt.xlabel('Segment')
plt.ylabel('Score de l\'émotion dominante')
plt.title('Évolution de l\'émotion dominante au fil de l\'histoire')
plt.xticks(x)  # Afficher les numéros de segment sur l'axe x
plt.grid(True)
plt.legend()
plt.tight_layout()

plt.show()




# Extract the list of emotions from the first segment
emotions = [entry['label'] for entry in emotionsS[0]]

# Extract scores for each emotion in each segment
scores = []
for segment in emotionsS:
    segment_scores = [entry['score'] for entry in segment]
    scores.append(segment_scores)

fig, ax = plt.subplots(figsize=(10, 6))

num_segments = len(emotionsS)
bar_width = 0.1

indices = np.arange(len(emotions))

for i in range(num_segments):
    ax.bar(indices + i * bar_width, scores[i], width=bar_width, label=f'Segment {i+1}')

# Set x-ticks in the middle of each group of bars
ax.set_xticks(indices + bar_width * (num_segments - 1) / 2)
ax.set_xticklabels(emotions)

# Add labels and title
ax.set_xlabel('Emotions')
ax.set_ylabel('Scores')
ax.set_title('Emotion Scores per Segment')
ax.legend(title='Segments')
plt.grid(True)

plt.show()



emotions = [entry['label'] for entry in emotionsS[0]]
scores_by_emotion = {emotion: [] for emotion in emotions}

for segment in emotionsS:
    for entry in segment:
        scores_by_emotion[entry['label']].append(entry['score'])

fig, ax = plt.subplots(figsize=(10, 6))

x = np.arange(1, len(emotionsS) + 1)  # x positions for the bars (1, 2, 3, ...)

bottom = np.zeros(len(emotionsS))  # Initialize the bottom for stacking
for emotion in emotions:
    scores = scores_by_emotion[emotion]
    bars = ax.bar(x, scores, bottom=bottom, label=emotion)
    bottom += scores

ax.set_xlabel('Segment')
ax.set_ylabel('Score')
ax.set_title('Emotion Scores per Segment (Stacked)')
ax.set_xticks(x)
ax.set_xticklabels([f'Segment {i}' for i in x])
ax.legend(title='Emotions', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)

plt.tight_layout()

plt.show()



df_metadata = pd.read_csv(src_folder + 'movie.metadata.tsv', sep='\t', header=None,
                                    names=['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_name', 
                                            'Movie_release_date', 'Movie_box_office_revenue', 'Movie_runtime',
                                            'Movie_languages_(Freebase ID:name tuples)', 'Movie_countries_(Freebase ID:name tuples)',
                                           'Movie_genres_(Freebase ID:name tuples)'])


df_summary_box_office = pd.concat([df_test_clean[['id','summary']], df_metadata['Movie_box_office_revenue']], axis=1)
print(df_summary_box_office.shape)
df_summary_box_office.dropna(axis=0, inplace=True)
df_summary_box_office.shape





src_save_emotion = 'df_emotion_per_movie'


#for every summary, compute its segment and every emotion for every segment, then store the result
classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Define a function to process each summary into segments and get emotion scores
def process_summary(summary, threshold=0.5):
    # Split the summary into sentences
    sentences = nltk.sent_tokenize(summary)
    sentences = [s.strip() for s in sentences if s]  # Remove empty sentences
    
    # Get embeddings and calculate similarities
    embeddings = model.encode(sentences)
    similarities = [util.pytorch_cos_sim(embeddings[i], embeddings[i+1]).item() for i in range(len(embeddings)-1)]
    
    # Split into segments based on similarity threshold
    segments = []
    current_segment = [sentences[0]]
    
    for i, similarity in enumerate(similarities):
        if similarity < threshold:
            segments.append(" ".join(current_segment))
            current_segment = []
        current_segment.append(sentences[i + 1])
    
    segments.append(" ".join(current_segment))  # Add the last segment

    # Get emotion scores for each segment
    segment_data = []
    for segment in segments:
        emotion_scores = {emotion['label']: emotion['score'] for emotion in classifier(segment)[0]}
        segment_data.append(emotion_scores)
    
    # Create DataFrame for this summary
    df_summary = pd.DataFrame(segment_data)
    return df_summary

# Assuming you have a main DataFrame df_summaries with a 'summary' column
# Process each summary and store the result as a dictionary of DataFrames
#summary_dfs = {}  # To store DataFrames for each summary
#df_summaries = df_summary_box_office.copy(deep=True)
df_summaries = df_test_clean.copy(deep=True)[:2000]
for _, row in tqdm(df_summaries.iterrows()):
    summary_text = row['summary']
    if not summary_text:
        continue
    df_summary = process_summary(summary_text)
    df_summary.name = row['id']
    path = src_save_emotion + f"/df_{df_summary.name}"
    df_summary.to_csv(path, index=False)
    #df_summary['box_office'] = row['Movie_box_office_revenue']
    #summary_dfs[idx] = df_summary  # Save the DataFrame by index or summary ID

# Display an example DataFrame for the first summary
#summary_dfs = list(summary_dfs.values())





import glob
import os

def load_dfs(folder_path,):

    file_paths = glob.glob(os.path.join(folder_path, '*')) 

    dataframes = []
    for file_path in tqdm(file_paths):
        #looks for id which should be at the end of the file
        match = re.search(r'df_(\d+)', file_path)
        name = int(match.group(1))
        df = pd.read_csv(file_path) 
        df.name = name
        dataframes.append(df)
    return dataframes
dfs = load_dfs(src_save_emotion)


src_save_emotion = "df_genre_and_emotions_per_movie"


def clean_genre(movie_genre):
    cleaned_text = re.findall(r': "([^"]+)"', movie_genre)
    return cleaned_text
def map_id_to_genre(df):
    id_ = df.name
    idx_movie = np.where(df_metadata['Wikipedia_movie_ID'] == id_)[0]
    if len(idx_movie) == 0:
        return []
    movie_genre = df_metadata['Movie_genres_(Freebase ID:name tuples)'].iloc[idx_movie].iloc[0]
    movie_genre_cleaned = clean_genre(movie_genre)
    return movie_genre_cleaned


#on prend chaque df, et on le duplicate autant de fois qu'il y a de genre associé à son film
new_dfs = []
features = list(dfs[0].columns)
features.append('genre')
genres = []
for idx, df in tqdm(enumerate(dfs)):
    movie_genre = map_id_to_genre(df)
    if len(movie_genre) == 0:
        continue
    df_result = pd.DataFrame()
    for genre in movie_genre:
        genre = genre.replace("/", "_").replace(" ", "_").replace("-", "_").replace("'", "_").replace("[", "_").replace("]", "_")
        df_result = df.copy(deep=True)
        df_result['genre'] = genre
        genres.append(genre)
        #df_result = pd.concat([df_result, df_concat], axis=0)
        #df_result = pd.get_dummies(df_result[features])
        df_result.name = df.name
        new_dfs.append(df_result)


#save every df with emotion with the genre of its film + keep every different genre we encounter
genres = []
for df in tqdm(new_dfs):
    genre = df['genre'].iloc[0]
    genres.append(genre)
    path = src_save_emotion + f"/df_{df.name}_{genre}"
    df.to_csv(path, index=False)
    


#get the counts of every different genre, and keep the one that are present more than n times
unique_genre, counts_genre = np.unique(genres, return_counts = True)
idx_genre_with_enough_film = np.where(counts_genre >= 20)
unique_genre = unique_genre[idx_genre_with_enough_film]
len(unique_genre)


#load les df que pour les genres qui ont assez de film
import glob
import os

def load_dfs_from_genre(folder_path):

    file_paths = glob.glob(os.path.join(folder_path, '*')) 

    dataframes = []
    for file_path in tqdm(file_paths):
        match = re.search(r'_(\D\w+)$', file_path)
        name = match.group(1)
        if name not in unique_genre:
            continue
        df = pd.read_csv(file_path) 
        df.name = name
        dataframes.append(df)
    return dataframes
dfs_with_genre = load_dfs_from_genre(src_save_emotion)


#mtn faut faire les calcules par genre
dfs_with_genre


glob.glob(os.path.join(src_save_emotion, '*'))[647:650]



